Model choice:
- RF because high number of observations vs. features
- it is a low bias, high variance method
- also could try knn or kernel svm

- also has good interpretability vs. neural nets & svm (and qventus does not use nn yet)
- lasso is also highly interpretable but maybe not as accurate
- light gradient boosted machine is another option
- rf and kernel svm allow data to not be linear

With more time, I would:
- use full data (sample = 1)
- tune more parameters/more options for parameters
- try automated hyperparameter tuning, like gradient descent, bayesian optimization (Hyperopt library), or evolutionary algorithms
- could do grid search after random search is narrowed down, but probably more benefit would come from feature engineering
- feature engineering
- because feature importance tends to inflate importance of high cardinality categorical variables and continuous, would try permutation importance with more computational power This shows how does random reshuffling of the data affect model performance?